В данной статье приводится обзор семейства технологий
RAID и обсуждаются детали реализации RAID-6 алгорит-
мов для платформы Intel 64. Производится сравнение систем
с открытым кодом, используемых для построения дисковых
массивов. Предлагается новый способ реализации алгорит-
мов RAID-6, позволяющий увеличить скорость вычислений
по сравнению с существующими промышленными система-
ми. Приведены фрагменты программного кода, разработан-
ного в соответствии с этим способом, представлены резуль-
таты измерений быстродействия алгоритмов.
Ключевые слова: cистемы хранения данных, дисковые массивы,
RAID, параллельные вычисления, SSE, AVX.
Введение
Многие современные промышленные программно-аппаратные си-
стемы, такие как системы управления ресурсами предприятия, си-
стемы оперативного анализа данных, системы управления цифро-
вым контентом и др., требуют активного обмена данными между
компьютерами и внешними накопителями данных. Однако извест-
но, что быстродействие таких накопителей (жёстких дисков) значи-
тельно ниже, чем быстродействие оперативной памяти компьюте-
ра, а производительность всей системы, как правило, определяется
быстродействием самого медленного её звена. В связи с этим возни-
кает задача увеличения скорости доступа к данным, хранимым на
внешних устройствах. Поэтому на практике широкое распростра-
нение получили подсистемы хранения данных (СХД), объединя-
ющие несколько независимых дисков в единое логическое устрой-
ство. Для повышения производительности в состав СХД включают-
ся несколько дисковых накопителей с возможностью параллельного
чтения и записи информации. В настоящее время активно развива-
ется семейство технологий под общим названием RAID (Redundant
Array of Independent/Inexpencive Disks —избыточный массив неза-
висимых или недорогих жёстких дисков) [1].
Эти технологии решают не только задачу повышения произво-
дительности СХД, но и сопутствующую ей задачу— повышение на-
дёжности хранения данных: ведь отдельные диски могут выходить
из строя при работе системы. Последняя задача решается введе-
нием информационной избыточности— используются дополнитель-
ные диски, на которые записываются специальным образом вычис-
ленные контрольные суммы, позволяющие восстановить информа-
цию в случае выхода из строя одного или нескольких дисков СХД.
Введение избыточных дисков позволяет решить проблему надёжно-
сти, но влечёт за собой необходимость выполнять дополнительные
действия, связанные с вычислением контрольных сумм при каждом
чтении/записи данных с дисков. Производительность этих вычис-
лений, в свою очередь, оказывает существенное влияние на про-
изводительность СХД в целом. Поэтому задача увеличения про-
изводительности RAID-вычислений является актуальной, при этом
наиболее востребованной на практике является технология RAID-6,
позволяющая восстанавливать два вышедших из строя диска1.
В этой статье приводится обзор семейства технологий RAID и
обсуждаются детали реализации RAID-6 алгоритмов на платформе
Intel 64. Несмотря на то, что данная информация опубликована в
открытых источниках [2,3], здесь она представлена в компактном и
целостном виде, удобном как для целей настоящего исследования,
так и для людей, которые хотят больше узнать о данном вопросе,
но не имеют возможности внимательно читать первоисточники. В
работе также представлен результат сравнения СХД с открытым
кодом «АВРОРА», в разработке которой принимал участие автор,
и системы Open-E, которая является наиболее распространённой
на рынке коммерческой СХД аналогичного класса. Эта информа-
ция представляет интерес в силу того, что сравнение алгоритмов,
реализующих технологию RAID-6, затруднено вследствие закры-
тости компаниями-производителями технических деталей. Кроме
того, данные сравнения СХД «АВРОРА» и Open-E используются
для обоснования эффективности основного результата данной ста-
тьи—нового способа организации параллельных вычислений при
реализации технологии RAID-6. Этот способ основывается на идее
«побитового параллелизма», позволяющего увеличить размер об-
ласти данных, обрабатываемой параллельно при вычислении кон-
трольных сумм.
Обзор
Семейство технологий RAID
Вычислительные алгоритмы, которые используются при построе-
нии RAID-массивов, появлялись постепенно и впервые были клас-
сифицированы в 1993 году в работе [1]. В соответствии с этой
классификацией массивом RAID-0 именуется массив независимых
дисков, в котором не предпринимаются меры по защите информа-
ции от утраты. Преимущество таких массивов по сравнению с оди-
ночным диском состоит в возможности существенного увеличения
ёмкости и производительности за счёт организации параллельного
обмена данными. Технология RAID-1 подразумевает дублирование
каждого диска системы. Таким образом, массив RAID-1 имеет удво-
енное количество дисков по сравнению с RAID-0, но выход из строя
одного диска системы не влечёт за собой утрату данных, посколь-
ку в массиве для каждого диска имеется его копия. Технологии
RAID-2 и RAID-3 не получили распространения на практике, и мы
опустим их описание.
Технология RAID-4 подразумевает использование одного до-
полнительного диска (стндрома), на который записывается сумма
остальных дисков данных СХД:
где N – это количество дисков с данными, а Di —содержимое i-го
диска. Эта контрольная сумма обновляется при выполнении каж-
дой записи данных на диски СХД2. Отметим, что при этом нет
необходимости вычислять (1) заново, а достаточно прибавить к
синдрому разность старого и нового значений изменяемого диска.
В случае выхода из строя одного из дисков уравнение (1) может
быть решено относительно появившегося неизвестного, т. е. данные
с утраченного диска будут восстановлены.
Очевидно, что операции чтения и записи синдрома происходят
чаще, чем любого другого диска данных. Этот диск становится са-
мым загруженным элементом массива, т. е. слабым звеном с точ-
ки зрения производительности (и, кстати, быстрее изнашивается).
Для решения этой проблемы были предложена технология RAID-5,
в которой для хранения синдромов используются части различных
дисков системы (рис. 1) и, тем самым, загрузка дисков операциями
чтения и записи выравнивается.
Отметим, что технологии RAID-1— RAID-5 позволяют восста-
навливать данные в случае выхода из строя одного из дисков, но в
случае утраты двух дисков эти технологии оказываются бессильны.
Конечно, вероятность одновременного выхода из строя двух дисков
значительно ниже, чем одного. Но на практике, во-первых, заме-
на отказавшего диска требует времени, в течение которого данные
остаются «беззащитными». Этот интервал может оказаться непри-
емлемо долгим в случае односменной работы системных админи-
страторов или в случае расположения системы в труднодоступном
месте. Во-вторых, при механической замене диска нельзя исклю-
чить возможность человеческой ошибки, т. е. ошибочной замены
исправного диска вместо неисправного— и мы снова имеем задачу
по восстановлению двух дисков. Для решения этих проблем была
предложена технология RAID-6, ориентированная на восстановле-
ние двух дисков. Рассмотрим эту технологию более подробно.
Технология RAID-6
Алгоритмы вычислений, используемые при построении СХД по
спецификации RAID-6, приведены в [2]. Здесь мы изложим их в
форме, удобной для использования в рамках настоящего исследо-
вания.
С целью увеличения производительности системы данные, по-
ступающие для записи, обычно накапливаются во внутреннем кэше
системы хранения данных и записываются на диски в соответствии
с внутренней стратегией кэширования, которая существенно влия-
ет на производительность системы в целом. При этом операции за-
писи выполняются большими массивами, которые будем называть
страйпами (Stripe). Аналогично записи, при запросе на чтение фи-
зически с дисков производится чтение не только запрошенных дан-
ных, но и всего страйпа (или нескольких страйпов), в котором эти
данные расположены. В дальнейшем страйп остается в кэше систе-
мы в ожидании относящихся к нему запросов на чтение. Для повы-
шения производительности СХД страйп записывается и читается
параллельно со всех дисков системы. Для этого он разбивается на
блоки одинакового размера, которые будем обозначать D0, ...Dn−1.
Количество блоков N равно количеству дисков данных в массиве.
Для обеспечения отказоустойчивости в дисковый массив вводятся
два дополнительных диска, которые будем обозначать P и Q. И в
страйп включим блоки, соответствующие массивам D0, ...Dn−1, а
также дискам P и Q (рис. 2). Два последних блока являются син-
дромами. В случае выхода из строя одного или двух дисков СХД
данные в соответствующих блоках восстанавливаются с использо-
ванием синдромов.
Отметим, что в RAID-6 для поддержания равномерной нагруз-
ки дисков, так же как и в технологии RAID-5, синдромы разных
страйпов помещаются на разные физические диски. Но для наше-
го исследования этот факт не имеет значения. Будем в дальнейшем
считать, что все синдромы хранятся в последних блоках страйпа.
Для вычисления синдромов разобьём блоки на отдельные слова,
и будем повторять вычисления контрольных сумм для всех слов
с одинаковыми номерами3. Для каждого слова будем вычислять
синдромы по следующему правилу:
где N —количество дисков в системе, Di —блок данных, соответ-
ствующий i-тому диску, P и Q— синдромы, qi —произвольные ко-
эффициенты.
Тогда в случае утраты дисков с номерами α и β можно составить
следующую систему уравнений:
где i = 0..N−1, i = α, β, α = β. Если система однозначно разрешима
для любых α и β, то в страйпе можно восстановить два любых
утраченных блока. Введём следующие обозначения:
Тогда имеем:
Для однозначной разрешимости (3) необходимо обеспечить, что-
бы все qα и qβ были различны и чтобы их разность была обратима
в той алгебраической структуре, в которой производятся вычис-
ления. Если в качестве такой структуры выбрать конечное поле
GF(2n)4, то оба эти условия совпадают. Если из двух вышедших из
строя дисков один содержал данные, а другой— синдром, то восста-
новить оба диска можно при помощи уцелевшего синдрома. Введём
следующие обозначения:
Тогда для восстановления утраченного диска Dα и синдрома Q
при помощи синдрома P получим следующую систему уравнений:
А для восстановления утраченного диска Dα и P при помощи
синдрома Q нужно решить следующую систему
Поскольку на практике количество дисков в массиве не очень
велико (редко превышает 100), то для повышения производитель-
ности мы можем заранее вычислить всевозможные необходимые
константы (qα − qβ)−1, qα, qα
−1 и использовать их в дальнейшем
при вычислениях.
Скорость выполнения вычислений в рассматриваемой задаче
критически важна для поддержания общей производительности
СХД не только в случае выхода из строя двух дисков, но и в случае
«штатного» режима работы, когда все диски работоспособны. Это
связано с тем, что страйп разбит на блоки, которые физически рас-
полагаются на разных дисках. Для чтения страйпа целиком ини-
циируется операция параллельного чтения блоков со всех дисков
системы. Когда все блоки прочитаны, из них собирается страйп и
операция чтения страйпа считается завершенной. При этом время
чтения страйпа определяется временем чтения последнего блока.
Таким образом, деградация производительности одного диска вле-
чёт за собой деградацию производительности всей системы. Кроме
этого на ухудшение времени чтения одного диска влияют такие
факторы, как неудачное расположение головки, случайно увели-
ченная нагрузка на диск, исправляемые электроникой внутренние
ошибки диска и т. д. Для решения этой проблемы имеется возмож-
ность не дожидаться завершения операции чтения с самых медлен-
ных дисков системы, а вычислить их значение по формулам (3), (4)
или (5). Но подобные вычисления будут полезны в рассматриваемой
ситуации, только если они могут выполняться достаточно быстро.
Отметим, что в приведённых рассуждениях для нас важно, что но-
мер отказавшего диска нам заведомо известен. С практической точ-
ки зрения это означает, что факт выхода из строя какого-либо диска
обнаруживается системами аппаратного контроля, и нам нет необ-
ходимости устанавливать его номер. Задача обнаружения «скрытой
потери» данных, т. е. искажения данных на дисках, которые счита-
ются системой работоспособными, лежит за рамками настоящего
исследования и подробно обсуждается, например, в [5].
Арифметические операции в конечных полях
Для более детальной оценки трудоёмкости вычислений в техноло-
гии RAID-6 и для поиска возможностей уменьшить их сложность
необходимо рассмотреть более подробно вычисления в конечных
полях. Мы будем рассматривать конечные поля вида GF(2n) (так
называемые поля Галуа), состоящие из 2n элементов. Следуя [6],
будем представлять элементы поля GF(2n) как многочлены с дво-
ичными коэффициентами степени не выше n−1. Такие многочлены
удобно записывать в виде машинных слов разрядности n. Будем за-
писывать их в 16-ричной системе счисления, например:
Известно, что для любого n поле GF(2n) получается путём фак-
торизации кольца многочленов над GF(2) по модулю неприводи-
мого многочлена степени n. Будем называть такой многочлен по-
рождающим. Таким образом, сложение в поле GF(2n) можно вы-
полнять как операцию сложения многочленов, а умножение— как
операцию умножения многочленов по модулю порождающего мно-
гочлена. То есть результат умножения двух многочленов делится
на порождающий многочлен и остаток от этого деления оказывает-
ся конечным результатом умножения двух элементов поля GF(2n).
Обширный список неприводимых многочленов можно найти
в [7]. Например, в качестве порождающего многочлена для поля
GF(28) может быть выбран многочлен x8 +x6 +x5++x4 +1, кото-
рый в шестнадцатеричной системе исчисления представляется как
число 171.
Операция сложения в GF(2n) выполняется одинаково и не зави-
сит от выбора порождающего многочлена, поскольку степень сум-
мы не может превышать наибольшую из степеней слагаемых, на-
пример:
В случае, когда степень порождающего многочлена не превы-
шает разрядности машинного слова, операция сложения элементов
поля выполняется за одну машинную команду поразрядного «ис-
ключающего или».
Операция умножения выполняется в два этапа: элементы поля
умножаются как многочлены, а затем находится остаток от деления
этого произведения на порождающий многочлен, например:
При этом в пересчёте на элементарные машинные операции
необходимо произвести до 2(n−1) сложений в зависимости от зна-
чения сомножителей. Именно в этой «зависимости» и находится
существенный резерв повышения производительности вычислений.
Например, если выбрать qi = xN−i−  1, то вычисление сумм вида
qiDi можно производить по схеме Горнера:
т. е. при вычислении синдромов P и Q в качестве сомножителя при
операции умножения можно зафиксировать многочлен x. Умноже-
ние на многочлен x сводится к операции сдвига на один разряд
влево и сложения результата с модулем, если при сдвиге произо-
шёл перенос, например:
В литературе [8] операция умножения на x часто изображается
в виде циклического регистра сдвига с обратными связям (рис. 3).
Здесь каждый квадратик обозначает разряд регистра, в который
записан коэффициент многочлена, соответствующий степени, рав-
ной номеру разряда. Стрелки показывают направление операции
сдвига регистра при умножении на x. При этом если в старшем би-
те находилась единица, то при помощи обратных связей она будет
прибавлена по модулю два к тем разрядам, которые соответствуют
единичным коэффициентам порождающего многочлена, т. е. про-
изойдёт сложение результата умножения на x с модулем, по кото-
рому построено поле.
С учётом выбора qi = xN−i−1 формулы (2), (3), (4), (5) при-
нимают следующий вид: (2) —вычисление синдромов; (3)— вос-
становление двух утраченных дисков; (4)—восстановление одного
диска данных Dα и синдрома Q при помощи синдрома P; (5) —
восстановление одного диска данных Dα и синдрома P при помо-
щи синдрома Q.
Операцию умножения двух произвольных элементов поля, кото-
рые являются многочленами степени меньше n, можно переписать
в следующем виде:
Переписанная в таком виде операция умножения более удобна
для программной реализации, поскольку сводится к последователь-
ности операций умножения на x и сложения. Это удобно потому,
что при вычислении произведения многочленов данным способом
промежуточные результаты не выходят за границу машинного сло-
ва. На рис. 4 представлены алгоритмы для вычисления (2)–(5).
Алгоритм расчёта синдромов и алгоритмы восстановление утра-
ченных дисков являются весьма похожими: каждый из них начи-
нается с обнуления рабочих переменных, в которых впоследствии
будут накапливаться суммы, обозначенные в формулах как P, Pα,
Pα,β, Q, Qα, Qα,β. Вычисление сумм производится в циклах, кото-
рые повторяются N раз, где N —число дисков в массиве. Циклы во
всех рассмотренных случаях являются и содержат только операции
сложения и умножения на x. Это означает, что они имеют одина-
ковое время исполнения. Алгоритмы на рис. 4 отличаются друг от
друга окончаниями, лежащим за пределами главного цикла. Тело
циклов (а значит, и операция умножения на x) повторяется N раз.
Операция умножения произвольных элементов поля, используемая
в окончаниях алгоритмов, сводится к повторению операции умно-
жения на x n раз, где n— степень поля. При N >> n основная
трудоёмкость вычислений сосредотачивается в циклах.
Увеличение скорости вычислений путём исполь-
зования векторной архитектуры Intel 64
На сегодняшний день самую развитую инфраструктуру поддерж-
ки и продаж имеют компьютерные системы, созданные на плат-
форме Intel 64. Поэтому естественно выбрать архитектуру Intel
64 в качестве базовой для разработки СХД. Векторные вычисле-
ния на базе процессоров Intel 64 описываются в [3] и содержат
спецификации SSE и AVX. Спецификация SSE позволяет исполь-
зовать шестнадцать 128-битных регистров процессора, именуемых
XMM0:XMM15. Каждый регистр может быть разбит на несколько
слов, в которых могут храниться числа с фиксированной или пла-
вающей точкой, например, на 16 целочисленных слов длиной 1 байт
каждое. Операции со всеми словами регистра производятся парал-
лельно. Спецификация AVX расширяет регистры XMM0:XMM15
до 256-битных регистров YMM0:YMM15. При этом они могут быть
разбиты на слова с плавающей точкой, и минимальный размер сло-
ва составляет 32 бита.
В разделе 1.2. обсуждалось, что для организации вычислений
необходимо, чтобы все элементы qi в формуле (2) были различны
и ни один не равен нулю. Поскольку i принимает значение от 0
до N − 1, то нам будет достаточно того, чтобы в конечном поле,
в котором будут производиться вычисления, было как минимум N
ненулевых элементов.
На практике редко встречаются дисковые массивы, состоящие
более чем из N = 100 дисков, поэтому для организации вычисле-
ний оказывается достаточным использовать поле GF(28). Это поле
состоит из (28) = 256 элементов, из которых 255 являются ненуле-
выми.
Поскольку элементы поля GF(2n) представляются байтами, а
размер машинного слова современного процессора существенно
больше, возникает естественная идея организации параллельной
(векторной) обработки нескольких элементов поля одновременно.
СХД «АВРОРА» использует спецификацию SSE для параллель-
ной обработки 16-ти элементов поля GF(28) на одном регистре. При
этом с учётом наличия 16 регистров и необходимости расчёта двух
синдромов, удаётся в цикле параллельно производить вычисления
с 64-мя элементами поля. Четыре регистра выделяются для парал-
лельного вычисления синдрома P, четыре—для вычисления син-
дрома Q, а оставшиеся восемь используются как вспомогательные
(рис. 5).
На этом рисунке показано, как 64 синдрома Q размещаются
на четырёх регистрах XMM0:XMM3. Такое размещение позволя-
ет производить параллельные вычисления 64-х синдромов.
Спецификация макрокоманды, осуществляющей параллельное
умножение на x 16-ти элементов, хранящихся на одном регистре,
представлена на рис. 6. В качестве первого параметра макрокоман-
ды указывается регистр, на котором размещены 16 однобайтных
элементов поля GF(2n).
Данная макрокоманда выделяет элементы со старшим единич-
ным битом и формирует для них маску, состоящую из значения
модуля. Затем сдвигает все байты регистра влево на один раз-
ряд и прибавляет значение маски. При вычислениях макрокоман-
да использует три константы, которые необходимо предварительно
разместить на регистрах XMM13, XMM14, XMM15. С помощью
этой макрокоманды можно построить и операцию умножения про-
извольных элементов поля. Макрокоманда для умножения одного
регистра использует дополнительный рабочий регистр и регистры,
на которые перед вычислением загружаются некоторые констан-
ты. Тем не менее, с учётом наличия 16 регистров XMM, имеется
возможность параллельной независимой обработки нескольких ре-
гистров.
К сожалению, использовать 256-битные регистры YMM не пред-
ставляется возможным. Это связано с тем, что спецификация AVX
не поддерживает слова размером 1 байт и не имеет в своём составе
команды сдвига на один бит, которая необходима для построения
операции умножения на x.
Сравнение быстродействия коммерческих систем,
построенных на открытых алгоритмах RAID-6
Задача сравнения быстродействия алгоритмов реализации RAID-6
в коммерческих системах является труднореализуемой по причи-
нам, перечисленным далее.
1. Высокая производительность системы является важнейшим
конкурентным преимуществом, и реализация алгоритмов
вычислений, чаще всего, составляет коммерческую тайну
компании-производителя.
2. Корректное сравнение времени исполнения алгоритмов долж-
но производиться при их реализации на одинаковом оборудо-
вании и работе на однотипном наборе данных. Эти условия
почти невозможно выполнить, так как очень часто такие ал-
горитмы имеют аппаратную реализацию, т. е. по определению
привязаны к различному оборудованию.
3. Быстродействие алгоритмов является только одним из фак-
торов, определяющих быстродействие СХД-системы, а публи-
куются, в основном, данные о производительности системы в
целом. Таким образом, оказывается не возможным выделить
и сравнить алгоритмы, реализованные в различных системах.
Тем не менее, имеется возможность корректно сравнить произ-
водительность так называемых программных RAID-массивов, име-
ющих открытый исходный код. Их важное отличие от аппаратных
RAID-массивов состоит в том, что в их спецификации не заложе-
ны специализированные микросхемы и другие аппаратные элемен-
ты. Разные программные RAID-массивы могут быть собраны из
одинаковых аппаратных компонент, что позволяет корректно срав-
нить быстродействие используемых ими алгоритмов для дальней-
шего анализа и оптимизации.
В данной работе в качестве RAID-массивов для сравнения
выбраны программные массивы СХД-систем с открытым кодом
Open-E и «АВРОРА». Этот выбор обусловлен тем, что Open-
E на сегодняшний день является лидером рынка программных
RAID-массивов, а система «АВРОРА» претендует на звание самого
высокопроизводительного программного RAID-массива. Компания
Open-E основана в 1998 году и имеет на сегодняшний день более
24000 инсталляций своего продукта в более чем 100 странах. Фи-
лософией компании является превращение систем хранения дан-
ных из уникальных дорогих систем в системы общего назначения.
На рынок компания поставляет две СХД: одна распространяется
бесплатно и предназначена для использования в малом бизнесе,
другая—для средних организаций. Оба продукта ориентированы
на оборудование архитектуры Intel. СХД «АВРОРА» с 2007 года
разрабатывалась в Петербурге компанией Digital Design, а с 2011
года— компанией Radix (Сколково). Продукт основан на схожих с
Open-E идеях, но основной упор делается на высокую производи-
тельность. Поэтому этот продукт оказывается востребованным, в
первую очередь, на рынке кино- и телеиндустрии, где постоянно
возрастают требования к количеству пикселов на кадр и, как пря-
мое следствие, к скорости обмена данными. Данный продукт также
ориентирован на оборудование архитектуры Intel.
Схожесть требований к аппаратной платформе СХД «АВРО-
РА» и Open-E даёт возможность при проведения испытаний устано-
вить обе системы на почти одинаковое оборудование и, тем самым,
добиться корректности измерений. Список используемого в наших
экспериментах оборудования приведён в таблице 1. И в том и в
другом случае выбраны широко распространённые на рынке компо-
ненты, а единственное различие касается реализации подключения
к серверу по протоколу Fiber Channel. Используемые компоненты
имеют сходные характеристики, но СХД «АВРОРА» позволяет ис-
пользовать только сетевые карты, производимые компанией ATTO,
а Open-E—компанией Qlogic.
При замерах производительности моделировалась работа СХД
для трёх различных случаев: при реализации файлового сервера,
базы данных и Web-сервера. Эти варианты соответствуют самым
распространённым случаям использования СХД. Моделирование
запросов ввода-вывода для всех вариантов производилось с исполь-
зованием так называемых шаблонов типовой загрузки в соответ-
ствии с рекомендациями [4]. Измерялись следующие параметры:
• количество операций ввода-вывода в секунду (IOps);
• количество переданных мегабайт в секунду (MBps).
Первый параметр характеризует производительность системы с
точки зрения скорости поиска и получения информации. Он важен
для транзакционных систем, которые обращаются к базе данных
с большим количеством запросов на чтение и запись небольших
блоков информации. Второй параметр важен при чтении и записи
блоков данных большого размера в системах, хранящих последо-
вательные файлы большого размера, например кинофильмы или
резервные копии данных. На рис. 7 представлены результаты из-
мерений производительности систем Open-E и «АВРОРА».
На рис. 7, а представлен случай моделирования файлового сер-
вера, на рис. 7, б— базы данных, на рис. 7, в—Web-сервера. На
каждой диаграмме для сравнения в графическом виде представле-
ны результаты измерений для систем Open-E и «АВРОРА». При
этом по горизонтальной оси отложены результаты измерений в
осуществленных операциях ввода-вывода в мегабайтах в секунду
(MBps):
Результаты измерений показывают многократное превосходство
СХД «АВРОРА» по обоим показателям для всех сценариев исполь-
зования. Этот результат позволяет нам утверждать, что техниче-
ские решения, которые позволяют увеличить быстродействие СХД
«АВРОРА», отличаются новизной и опережают возможности дру-
гих современных СХД, построенных по технологии RAID-6.
«Побитовый параллелизм»
в реализации технологии RAID-6
Существующая реализация СХД «АВРОРА» при вычислениях
разбивает 128-битные регистры SSE на 16 однобайтных элементов,
каждый из которых содержит один элемент поля GF(28). При этом
удаётся организовать параллельную обработку 64-байтной линии.
Для обработки, например, 4K-байтного блока приходится повто-
рять вычисления в цикле 64 раза (4K/64 = 64).
В данном исследовании предлагается использовать «побито-
вый параллелизм» —альтернативный способ организации парал-
лельных вычислений, позволяющий увеличить ширину обрабаты-
ваемой линии и, за счёт этого, увеличить скорость вычислений. В
отличие от способа, при котором регистр процессора разбивается на
части, каждая из которых содержит элемент поля GF(28), предла-
гается каждый элемент поля разделить на восемь битов, и каждый
из них записать на различные регистры YMM (рис. 8). Данный спо-
соб позволяет разместить на восьми регистрах YMM процессора
Intel 64 256 элементов поля GF(28).
Операция сложения 256-ти элементов поля GF(28) выполняется
как операция сложения по модулю два восьми пар регистров. Все
восемь сложений являются независимыми друго т друга и испол-
няются параллельно на внутренних конвейерах арифметического
устройства процессора Intel 64, как показано на рис. 9.
Операция параллельного умножения 256 элементов GF(28) на
элемент выполняется путём комбинации пересылок и сложений 256-
разрядных слов, как это показано на рис. 10. Отличие от представ-
ленной на рис. 3 схемы умножения на x одного элемента GF(28)
заключается в том, что в данном случае операции сдвига и сложе-
ния осуществляются одновременно и независимо с 256-ю битами.
На рис. 11 представлен вариант реализации этой операции на
языке ассемблера. Реализация состоит из трёх макрокоманд. Пер-
вая макрокоманда YMM_CHG осуществляет обмен содержимого
двух регистров, переданных ей в качестве параметров. Обмен осу-
ществляется с помощью операций сложения по модулю два [9].
Такой способ обмена позволяет избежать использования допол-
нительного регистра, что даёт возможность использовать по во-
семь регистров для вычисления двух синдромов P и Q с учё-
том наличия в спецификации AVX шестнадцати 256-разрядных
регистров. Макрокоманда YMM0_YMM7_ROTATE осуществля-
ет обмен информацией регистров YMM0:YMM7 «по кольцу» в со-
ответствии с тем, как изображено на рис. 10. Это осуществляет-
ся путем восьмикратного повторения макрокоманды YMM_CHG
с регистрами YMM0:YMM7 в качестве операндов. Макрокоман-
да GF8x256_MULX осуществляет сдвиг« по кольцу» восьми ре-
гистров с помощью макрокоманды YMM0_YMM7_ROTATE и за-
тем складывает по модулю 2 регистр YMM0 с регистрами YMM4,
YMM5 и YMM6, как показано на рис. 10. Тем самым выполняется
умножение на 256-ти элементов.
Таким образом, предложенный способ «побитового параллелиз-
ма» позволяет осуществлять параллельную обработку 256-ти эле-
ментов поля (256-ти байт), в отличие от примеряемого в СХД
«АВРОРА» алгоритма, осуществляющего параллельную обработ-
ку только 64-х элементов (64 байта). Для расчёта страйпа, состав-
ленного из блоков размером, например, 4K, необходимо повторение
главного цикла всего 16 раз в отличие от алгоритма, применяемого
в системе «АВРОРА», который требует 64-х повторений.
Реализация и достигнутые результаты
В рамках проведённого исследования были реализованы функции
вычисления синдромов и восстановления утраченных дисков в соот-
ветствии со спецификацией, используемой в СХД «АВРОРА». При
этом блоки, из которых составлен страйп, были выбраны размером,
равным 4 K, что соответствует размеру страницы в OC Linux. Та-
кой выбор размера блока обусловлен тем, что программный RAID-
массив реализуется как набор модулей, исполняющихся в «режиме
ядра» операционной системы Linux, а в режиме ядра возможно вы-
деление и использование блоков памяти только страницами по 4 K.
Были проведены измерения времени исполнения двух алгоритмов:
алгоритма, используемого в текущей версии СХД «АВРОРА», и
нового алгоритма, использующего «побитовый параллелизм». При
проведении измерений моделировалась ситуация выхода из строя
двух дисков со случайными номерами. Среди этих дисков могли
оказаться как диски данных, так и синдромы. Поэтому в качестве
измеряемой вычислительной операции случайным образом могли
выступать как операции вычисления одного или двух синдромов,
так и операции восстановления дисков. Данная модель эксперимен-
та наиболее точно соответствует реальной эксплуатации СХД.
В эксперименте производилось измерение времени вычисления
в зависимости от количества дисков в массиве. Время вычисления
измерялось в машинных циклах (тиках), что позволило абстрагиро-
ваться от тактовой частоты процессора. Измерения производились
серии по 100, при этом 20% крайних результатов отбрасывались, а
остальные усреднялись.
По результатам измерений построены графики зависимости вре-
мени вычислений от количества дисков (рис. 12). На рис. 12, a и б
изображена зависимость времени вычислений в тиках (вертикаль-
ная ось) от количества дисков в системе (горизонтальная ось). При
этом на рис. 12, а представлены результаты измерений для количе-
ства дисков от 5 до 25. Для построения графика измерения про-
изводились с шагом 1 диск. График, изображён на рис. 12, б пред-
ставляет ту же зависимость, но для его построения проводились
измерения времени исполнения для количества дисков от 28 до 128
с шагом 5 дисков. Поскольку количество повторений главного цик-
ла вычислений (см. рис. 4) равно количеству дисков, наблюдается
линейный рост времени вычислений с увеличением количества дис-
ков. При этом можно констатировать существенное превосходство
предлагаемого в настоящей статье метода от метода, используемого
в коммерческой реализации СХД «АВРОРА». Для наглядности на
рис. 12, в и г показаны те же самые зависимости, но время работы
с использованием предлагаемого метода кодирования изображено
в процентах от времени работы метода, используемого в настоящее
время.
Сравнение результатов измерений позволяет сделать вывод о
достигнутом шестикратном сокращении времени вычислений при
использовании предлагаемого алгоритма по сравнению с алгорит-
мом, применяемым в СХД «АВРОРА» в настоящее время.
Заключение
В настоящей работе приведён обзор СХД, основанных на RAID-
технологии и способных восстанавливать информацию при отказе
одного или двух дисков. Приведено сравнение производительности
систем с открытым кодом «АВРОРА» и Open-E. Предложен метод
«побитового параллелизма», который может быть использован при
вычислении синдромов и восстановлении утраченной информации.
Проведены измерения и продемонстрирована эффективность пред-
лагаемого метода, позволяющего увеличить скорость вычислений
в 6 раз по сравнению с методами, используемыми в настоящее вре-
мя. При предлагаемом способе кодирования вычисления сводятся к
элементарным операциям пересылки и побитного сложения по мо-
дулю два. Это позволяет использовать для вычислений более про-
стые устройства, чем центральный процессор архитектуры Intel 64,
например, видеокарту компьютера.
В дальнейшем предполагается вести работы в двух направлени-
ях. Во-первых, предлагаемый метод «побитового параллелизма» с
высокой степенью вероятности может быть без особых трудностей
использован в системах, построенных на базе процессора ARM-64,
появление которого ожидается в ближайшем будущем. Этот про-
цессор отличается от процессора архитектуры Intel боле низким
энергопотреблением, что имеет очень большое значение для цен-
тров обработки данных. Разумеется, низким энергопотреблением
должны отличаться и другие компоненты системы, которых сей-
час еще нет на рынке. Поэтому невозможно оценить эффективность
использования предлагаемого метода без дополнительных исследо-
ваний. Во-вторых, имеющий место рост ёмкости дисков и количе-
ства дисков в системе хранения повышает вероятность выхода из
строя дисков системы. Поэтому актуальным становится использо-
вание систем с количеством синдромов больше двух. Эффектив-
ность использования предлагаемого метода для систем с большим
количеством синдромов также требует дальнейшего исследования.